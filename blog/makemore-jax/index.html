<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="utf-8" />
	<link rel="icon" href="../../favicon.ico" />
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />

	<!-- Dark User Agent UI -->
	<meta name="color-scheme" content="dark light">

	<!-- Meta Info -->
	<meta property="og:title" content="Doga Tekin">
	<meta property="og:description" content="Doga Tekin's website.">
	<meta property="og:image" content="https://www.dogatekin.com/home.png">
	<meta property="og:url" content="https://www.dogatekin.com">

	<!-- Google Fonts -->
	<link rel="preconnect" href="https://fonts.gstatic.com">
	<link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,400;0,600;0,700;1,400;1,600;1,700&display=swap" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css2?family=PT+Sans&display=swap" rel="stylesheet">

	
		<link href="../../_app/immutable/assets/0.2yW4VHhZ.css" rel="stylesheet">
		<link href="../../_app/immutable/assets/2.DyiNw9h-.css" rel="stylesheet">
		<link href="../../_app/immutable/assets/8.BihBpRhf.css" rel="stylesheet">
		<link rel="modulepreload" href="../../_app/immutable/entry/start.Dd94DQFU.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/entry.X1zjEDRs.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/index-client.Og0LhCYB.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/runtime.wGhI371t.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/paths.B7GyUY4r.js">
		<link rel="modulepreload" href="../../_app/immutable/entry/app.v7VW2JF8.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/preload-helper.C1FmrZbK.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/render.BrLSkpY2.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/events.BCoeD6lw.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/svelte-head.BsjTvNUI.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/disclose-version.BdpnkpC4.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/if.CsQLbB-X.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/props.CvJs4P4I.js">
		<link rel="modulepreload" href="../../_app/immutable/nodes/0.BFb3hCJ1.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/snippet.CIQ3ynDX.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/attributes.B9wGpIMf.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/class.Bvena8yX.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/legacy.C9v6lNsv.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/lifecycle.Bv5KmAhi.js">
		<link rel="modulepreload" href="../../_app/immutable/nodes/2.oYwZNKm7.js">
		<link rel="modulepreload" href="../../_app/immutable/nodes/8.0TDIQtee.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/html.DhKoHQOW.js">
		<link rel="modulepreload" href="../../_app/immutable/chunks/each.DOlodmht.js"><!--[--><script>
		if (document) {
			let theme = localStorage.getItem('theme') || 'dark-theme';
			if (theme === 'light-theme') {
				document.documentElement.classList.add('light-theme');
			}
		}
	</script><!----><!--]--><!--[--><!--]--><title>Doga Tekin on Learning JAX</title>
</head>

<body data-sveltekit-preload-data="hover">
	<div style="display: contents"><!--[--><!--[--><!----><!----> <nav class="svelte-17o092z"><div id="left" class="svelte-17o092z"><a href="../../" class="svelte-17o092z"><div id="brand" class="svelte-17o092z"><div id="logo-holder" class="svelte-17o092z"><img src="../../logo.svg" alt="Logo" id="logo" class="svelte-17o092z"></div> Doga Tekin</div></a> <div id="icons" class="svelte-17o092z"><a href="mailto:dotekin@gmail.com" class="svelte-17o092z"><i class="fas fa-envelope"></i></a> <a target="_blank" rel="noreferrer" href="https://github.com/dogatekin" class="svelte-17o092z"><i class="fab fa-github"></i></a> <a target="_blank" rel="noreferrer" href="https://www.linkedin.com/in/dogatekin/" class="svelte-17o092z"><i class="fab fa-linkedin-in"></i></a>   <a class="toggler svelte-17o092z" role="button"><i class="fas fa-adjust"></i></a></div></div> <div id="links" class="svelte-17o092z"><a href="../../resume" class="svelte-17o092z">Resume</a> <a href="../../projects" class="svelte-17o092z">Projects</a> <a href="../../blog" class="svelte-17o092z">Blog</a> <a href="../../loops" class="svelte-17o092z">Loops</a> <a href="../../fun" class="svelte-17o092z">Fun</a></div> <div id="mobile-buttons" class="svelte-17o092z"><a class="toggler svelte-17o092z" role="button"><i class="fas fa-adjust"></i></a> <button class="text-gray-500 hover:text-gray-700 cursor-pointer mr-4 border-none focus:outline-none svelte-y5ffr1 "><svg width="32" height="24" class="svelte-y5ffr1"><line id="top" x1="0" y1="2" x2="32" y2="2" class="svelte-y5ffr1"></line><line id="middle" x1="0" y1="12" x2="32" y2="12" class="svelte-y5ffr1"></line><line id="bottom" x1="0" y1="22" x2="32" y2="22" class="svelte-y5ffr1"></line></svg></button><!----></div></nav><!----> <aside class="svelte-rfheuh "><div id="links" class="svelte-rfheuh"><a href="../../resume" class="svelte-rfheuh">Resume</a> <a href="../../projects" class="svelte-rfheuh">Projects</a> <a href="../../blog" class="svelte-rfheuh">Blog</a> <a href="../../loops" class="svelte-rfheuh">Loops</a> <a href="../../fun" class="svelte-rfheuh">Fun</a></div> <div id="icons" class="svelte-rfheuh"><a href="mailto:dotekin@gmail.com" class="svelte-rfheuh"><i class="fas fa-envelope"></i></a> <a target="_blank" rel="noreferrer" href="https://github.com/dogatekin" class="svelte-rfheuh"><i class="fab fa-github"></i></a> <a target="_blank" rel="noreferrer" href="https://www.linkedin.com/in/dogatekin/" class="svelte-rfheuh"><i class="fab fa-linkedin-in"></i></a></div></aside><!----> <main id="content"><!--[--><!----><article class="svelte-1er77a2"><!----><h1>Learning JAX by Implementing a Simple Language Model</h1> <p><a href="https://github.com/google/jax" rel="nofollow">JAX</a> is a cool machine learning framework from Google.
Language models (LMs) are the family of models that include ChatGPT and GPT-4, which have taken the machine learning world by storm.</p> <p>What better way to learn about both than to implement a simple LM in JAX?</p> <h2>Resources</h2> <p>Iâ€™ll heavily rely on Andrej Karpathyâ€™s amazing course <a href="https://karpathy.ai/zero-to-hero.html" rel="nofollow">Neural Networks: Zero to Hero</a> for this implementation.
The course starts from the very basics of neural networks and slowly builds up to implementing a GPT model.</p> <p>If you are interested in learning about all of these things in detail, I highly recommend checking the course out.
This blog post is only concerned about porting the implementation to JAX, and wonâ€™t go into the theoretical background of what is being implemented.</p> <p>As for JAX, Iâ€™ve found the <a href="https://jax.readthedocs.io/en/latest/index.html" rel="nofollow">official documentation</a> adequate so far.</p> <h2>Model</h2> <p>The model Iâ€™m interested in building is the Multi Layer Perceptron (MLP) model from <a href="https://www.youtube.com/watch?v=TCH_1BHY58I" rel="nofollow">Building makemore Part 2: MLP</a>.
I think this is a good starting point due to the simplicity of the model built in this chapter.</p> <p>Following the steps in this video, I will be building a character-level LM that looks at the previous three characters to predict which character will come next.</p> <h2>Implementation</h2> <p>While displaying the relevant code blocks, Iâ€™ll also provide the PyTorch counterparts in a separate tab for easier comparison.</p> <p>Letâ€™s start by initializing the model parameters:</p> <div class="tabs"><ul class="svelte-s2w0x0"><!--[--><li class="svelte-s2w0x0"><div class="svelte-s2w0x0 ">Torch</div></li><li class="svelte-s2w0x0"><div class="svelte-s2w0x0 active">JAX</div></li><!--]--></ul></div><!----> <!--[!--><!--[--><pre class="language-python"><!----><code class="language-python">alphabet_size <span class="token operator">=</span> <span class="token number">27</span>
context_size <span class="token operator">=</span> <span class="token number">3</span>
neurons <span class="token operator">=</span> <span class="token number">100</span>
d <span class="token operator">=</span> <span class="token number">2</span>

key <span class="token operator">=</span> jrandom<span class="token punctuation">.</span>PRNGKey<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
key<span class="token punctuation">,</span> <span class="token operator">*</span>subkeys <span class="token operator">=</span> jrandom<span class="token punctuation">.</span>split<span class="token punctuation">(</span>key<span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span>
subkeys <span class="token operator">=</span> <span class="token builtin">iter</span><span class="token punctuation">(</span>subkeys<span class="token punctuation">)</span>

C <span class="token operator">=</span> jrandom<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token builtin">next</span><span class="token punctuation">(</span>subkeys<span class="token punctuation">)</span><span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">(</span>alphabet_size<span class="token punctuation">,</span> d<span class="token punctuation">)</span><span class="token punctuation">)</span>
W1 <span class="token operator">=</span> jrandom<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token builtin">next</span><span class="token punctuation">(</span>subkeys<span class="token punctuation">)</span><span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">(</span>d <span class="token operator">*</span> context_size<span class="token punctuation">,</span> neurons<span class="token punctuation">)</span><span class="token punctuation">)</span>
b1 <span class="token operator">=</span> jrandom<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token builtin">next</span><span class="token punctuation">(</span>subkeys<span class="token punctuation">)</span><span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">(</span>neurons<span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">)</span>
W2 <span class="token operator">=</span> jrandom<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token builtin">next</span><span class="token punctuation">(</span>subkeys<span class="token punctuation">)</span><span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">(</span>neurons<span class="token punctuation">,</span> alphabet_size<span class="token punctuation">)</span><span class="token punctuation">)</span>
b2 <span class="token operator">=</span> jrandom<span class="token punctuation">.</span>normal<span class="token punctuation">(</span><span class="token builtin">next</span><span class="token punctuation">(</span>subkeys<span class="token punctuation">)</span><span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">(</span>alphabet_size<span class="token punctuation">,</span> <span class="token punctuation">)</span><span class="token punctuation">)</span>

parameters <span class="token operator">=</span> <span class="token punctuation">[</span>C<span class="token punctuation">,</span> W1<span class="token punctuation">,</span> b1<span class="token punctuation">,</span> W2<span class="token punctuation">,</span> b2<span class="token punctuation">]</span></code><!----></pre><!--]--><!--]--> <p>You can notice they are very similar.
Other than method and parameter names, the only difference is how JAX handles random number generation.
You can read more about that in the corresponding section of <a href="https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#random-numbers" rel="nofollow">ðŸ”ª JAX - The Sharp Bits ðŸ”ª</a>.</p> <p>Now letâ€™s write the forward pass:</p> <div class="tabs"><ul class="svelte-s2w0x0"><!--[--><li class="svelte-s2w0x0"><div class="svelte-s2w0x0 ">Torch</div></li><li class="svelte-s2w0x0"><div class="svelte-s2w0x0 active">JAX</div></li><!--]--></ul></div><!----> <!--[!--><!--[--><pre class="language-python"><!----><code class="language-python"><span class="token keyword">def</span> <span class="token function">model</span><span class="token punctuation">(</span>C<span class="token punctuation">,</span> W1<span class="token punctuation">,</span> b1<span class="token punctuation">,</span> W2<span class="token punctuation">,</span> b2<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
    emb <span class="token operator">=</span> C<span class="token punctuation">[</span>X<span class="token punctuation">]</span>
    h <span class="token operator">=</span> jnp<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>emb<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span> @ W1 <span class="token operator">+</span> b1<span class="token punctuation">)</span>
    logits <span class="token operator">=</span> h @ W2 <span class="token operator">+</span> b2
    logits <span class="token operator">-=</span> logits<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> keepdims<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    counts <span class="token operator">=</span> jnp<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>logits<span class="token punctuation">)</span>
    probs <span class="token operator">=</span> counts <span class="token operator">/</span> counts<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> keepdims<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> probs</code><!----></pre><!--]--><!--]--> <p>Again very similar.</p> <p>Now we need a loss function:</p> <div class="tabs"><ul class="svelte-s2w0x0"><!--[--><li class="svelte-s2w0x0"><div class="svelte-s2w0x0 ">Torch</div></li><li class="svelte-s2w0x0"><div class="svelte-s2w0x0 active">JAX</div></li><!--]--></ul></div><!----> <!--[!--><!--[--><pre class="language-python"><!----><code class="language-python"><span class="token keyword">def</span> <span class="token function">loss_fn</span><span class="token punctuation">(</span>C<span class="token punctuation">,</span> W1<span class="token punctuation">,</span> b1<span class="token punctuation">,</span> W2<span class="token punctuation">,</span> b2<span class="token punctuation">,</span> X<span class="token punctuation">,</span> Y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    probs <span class="token operator">=</span> model<span class="token punctuation">(</span>C<span class="token punctuation">,</span> W1<span class="token punctuation">,</span> b1<span class="token punctuation">,</span> W2<span class="token punctuation">,</span> b2<span class="token punctuation">,</span> X<span class="token punctuation">)</span>
    loss <span class="token operator">=</span> <span class="token operator">-</span>jnp<span class="token punctuation">.</span>log<span class="token punctuation">(</span>probs<span class="token punctuation">[</span>jnp<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> Y<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> loss</code><!----></pre><!--]--><!--]--> <p>Here literally the only change is replacing <code>torch</code> with <code>jnp</code>.</p> <p>And now the training function, which is the first major difference between PyTorch and JAX:</p> <div class="tabs"><ul class="svelte-s2w0x0"><!--[--><li class="svelte-s2w0x0"><div class="svelte-s2w0x0 ">Torch</div></li><li class="svelte-s2w0x0"><div class="svelte-s2w0x0 active">JAX</div></li><!--]--></ul></div><!----> <!--[!--><!--[--><pre class="language-python"><!----><code class="language-python"><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>C<span class="token punctuation">,</span> W1<span class="token punctuation">,</span> b1<span class="token punctuation">,</span> W2<span class="token punctuation">,</span> b2<span class="token punctuation">,</span> Xb<span class="token punctuation">,</span> Yb<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    grad_fn <span class="token operator">=</span> grad<span class="token punctuation">(</span>loss_fn<span class="token punctuation">,</span> argnums<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    gC<span class="token punctuation">,</span> gW1<span class="token punctuation">,</span> gb1<span class="token punctuation">,</span> gW2<span class="token punctuation">,</span> gb2 <span class="token operator">=</span> grad_fn<span class="token punctuation">(</span>C<span class="token punctuation">,</span> W1<span class="token punctuation">,</span> b1<span class="token punctuation">,</span> W2<span class="token punctuation">,</span> b2<span class="token punctuation">,</span> Xb<span class="token punctuation">,</span> Yb<span class="token punctuation">)</span>

    C <span class="token operator">-=</span> lr <span class="token operator">*</span> gC
    W1 <span class="token operator">-=</span> lr <span class="token operator">*</span> gW1
    b1 <span class="token operator">-=</span> lr <span class="token operator">*</span> gb1
    W2 <span class="token operator">-=</span> lr <span class="token operator">*</span> gW2
    b2 <span class="token operator">-=</span> lr <span class="token operator">*</span> gb2

    <span class="token keyword">return</span> C<span class="token punctuation">,</span> W1<span class="token punctuation">,</span> b1<span class="token punctuation">,</span> W2<span class="token punctuation">,</span> b2</code><!----></pre><!--]--><!--]--> <p>In PyTorch, we get a tensor from the loss function, which we can call <code>backward</code> on to backpropagate the gradients to our parameters.
Then, we can loop through all our parameters and update them in-place.
We also need to remember to clear up the gradients for each parameter after every iteration, otherwise they would accumulate.</p> <p>In JAX, we use <code>grad</code> on the loss function to obtain a new function that evaluates the gradients of that function with respect to the parameters we choose.
We then use that function to find the gradients at the current point in our parameter space, and use those gradients to update them.
Also notable: we need to return new arrays to replace the old ones, instead of updating them in-place as JAX follows a more functional paradigm than PyTorch.</p> <p>Even though I passed each parameter and received their gradients explicitly in this implementation, JAX also has ways to pass all parameters and receive all gradients together using <a href="https://jax.readthedocs.io/en/latest/jax-101/05.1-pytrees.html" rel="nofollow">pytrees</a>.</p> <p>Finally, the training loop:</p> <div class="tabs"><ul class="svelte-s2w0x0"><!--[--><li class="svelte-s2w0x0"><div class="svelte-s2w0x0 ">Torch</div></li><li class="svelte-s2w0x0"><div class="svelte-s2w0x0 active">JAX</div></li><!--]--></ul></div><!----> <!--[!--><!--[--><pre class="language-python"><!----><code class="language-python">key <span class="token operator">=</span> jrandom<span class="token punctuation">.</span>PRNGKey<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    key<span class="token punctuation">,</span> subkey <span class="token operator">=</span> jrandom<span class="token punctuation">.</span>split<span class="token punctuation">(</span>key<span class="token punctuation">)</span>
    ix <span class="token operator">=</span> jrandom<span class="token punctuation">.</span>randint<span class="token punctuation">(</span>subkey<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> minval<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> maxval<span class="token operator">=</span>Xtr<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    Xb<span class="token punctuation">,</span> Yb <span class="token operator">=</span> Xtr<span class="token punctuation">[</span>ix<span class="token punctuation">]</span><span class="token punctuation">,</span> Ytr<span class="token punctuation">[</span>ix<span class="token punctuation">]</span>
    C<span class="token punctuation">,</span> W1<span class="token punctuation">,</span> b1<span class="token punctuation">,</span> W2<span class="token punctuation">,</span> b2 <span class="token operator">=</span> train<span class="token punctuation">(</span>C<span class="token punctuation">,</span> W1<span class="token punctuation">,</span> b1<span class="token punctuation">,</span> W2<span class="token punctuation">,</span> b2<span class="token punctuation">,</span> Xb<span class="token punctuation">,</span> Yb<span class="token punctuation">)</span></code><!----></pre><!--]--><!--]--> <p>Again we see only minimal differences due to how the two libraries handle random number generation differently.</p> <h2>Data</h2> <p>Iâ€™ll use the same <a href="https://github.com/karpathy/makemore/blob/master/names.txt" rel="nofollow">dataset of 32K English names</a> as Andrej and split it into train / dev / test sets in the same way.</p> <h2>Training &amp; Evaluation</h2> <p>Here are the training and dev losses during training:</p> <div class="tabs"><ul class="svelte-s2w0x0"><!--[--><li class="svelte-s2w0x0"><div class="svelte-s2w0x0 ">Torch</div></li><li class="svelte-s2w0x0"><div class="svelte-s2w0x0 active">JAX</div></li><!--]--></ul></div><!----> <!--[!--><!--[--><p><img src="/_app/immutable/assets/jax_loss.CGj7eP2z.png" alt="jax-loss"></p><!--]--><!--]--> <p>These are the learned character embeddings:</p> <div class="tabs"><ul class="svelte-s2w0x0"><!--[--><li class="svelte-s2w0x0"><div class="svelte-s2w0x0 ">Torch</div></li><li class="svelte-s2w0x0"><div class="svelte-s2w0x0 active">JAX</div></li><!--]--></ul></div><!----> <!--[!--><!--[--><p><img src="/_app/immutable/assets/jax_emb.D7pPxac8.png" alt="jax-emb"></p><!--]--><!--]--> <p>These are a sample of names generated by the LM:</p> <div class="tabs"><ul class="svelte-s2w0x0"><!--[--><li class="svelte-s2w0x0"><div class="svelte-s2w0x0 ">Torch</div></li><li class="svelte-s2w0x0"><div class="svelte-s2w0x0 active">JAX</div></li><!--]--></ul></div><!----> <!--[!--><!--[--><ul><li>brone</li> <li>jieai</li> <li>krillo</li> <li>jrayle</li> <li>karireio</li> <li>addelie</li></ul><!--]--><!--]--> <h2>Performance</h2> <p>If I run 10000 steps of training on my CPU based on the code presented above, I get the following times:</p> <table><thead><tr><th>PyTorch</th><th>JAX</th></tr></thead><tbody><tr><td>5.6s</td><td>158s</td></tr></tbody></table> <p>But JAX has one more core functionality I didnâ€™t mention, Just In Time (JIT) compilation.
When a function is decorated with the JIT decorator, all the operations in that function get compiled into a more efficient version that can then be run on any input of the same shape and type.</p> <p>After we JIT the <code>train</code> function, we get:</p> <table><thead><tr><th>PyTorch</th><th>JAX</th><th>JAX (after JIT)</th></tr></thead><tbody><tr><td>5.6s</td><td>158s</td><td>5.9s</td></tr></tbody></table> <p>Nobody would normally train language models on MacBook CPUs, but this at least gives an idea on the importance of knowing how to use JIT when using JAX.</p> <h2>Conclusion</h2> <p>I believe this was a good introduction to JAX and to implementing language models.
A good next step would be to implement the more sophisticated LMs from later in the series, and using that as an opportunity to learn <a href="https://github.com/deepmind/dm-haiku" rel="nofollow">Haiku</a>!</p><!----><!----></article><!----><!--]--><!----></main> <footer></footer><!----><!--]--> <!--[!--><!--]--><!--]-->
			
			<script>
				{
					__sveltekit_9bi7o5 = {
						base: new URL("../..", location).pathname.slice(0, -1)
					};

					const element = document.currentScript.parentElement;

					Promise.all([
						import("../../_app/immutable/entry/start.Dd94DQFU.js"),
						import("../../_app/immutable/entry/app.v7VW2JF8.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 2, 8],
							data: [null,null,null],
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>

	<!-- FontAwesome -->
	<script src="https://kit.fontawesome.com/1f5e88f762.js" crossorigin="anonymous"></script>
</body>

</html>